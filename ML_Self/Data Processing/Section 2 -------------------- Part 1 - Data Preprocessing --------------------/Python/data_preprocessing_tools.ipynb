{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "37puETfgRzzg"
      },
      "source": [
        "# Data Preprocessing Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EoRP98MpR-qj"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import 3 libraries: numpy (work with arrays), matplotlib (plot charts), pandas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RopL7tUZSQkT"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import Data.csv\n",
        "dataset = pd.read_csv('Data.csv') \n",
        "# features (X): they are the columns using which you predict the dependent variable (first 3 columns in data set)\n",
        "# dependent variable (y): this is what is supposed to be predicted (Y) (last column in dataset)\n",
        "\n",
        "X = dataset.iloc[:, :-1].values\n",
        "#iLoc = indexes of columns. : will select all the rows in column and , will select first 3 columns, ignoring last column\n",
        "# -1 in python means last column. So :-1 will take all columns, excluding the last column\n",
        "\n",
        "y = dataset.iloc[:, -1].values\n",
        "# only extract last column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ],
      "source": [
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ]
        }
      ],
      "source": [
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nhfKXNxlSabC"
      },
      "source": [
        "## Taking care of missing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ],
      "source": [
        "## if we are missing some data, lets replace the data with average of all other data. Otherwise, this can lead to errors in our learning model.\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "## create instance of above class\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "# np.nan means we want to replace only empty values; strategy='mean' means we want to replace empty values with mean (average) value\n",
        "\n",
        "## apply imputer object to data; use of methods\n",
        "# 1:3 => we only select numerical columns to search for missing data (age and salary), since strings can cause errors\n",
        "imputer.fit(X[:, 1:3])\n",
        "\n",
        "## transform method => used to perform replacement\n",
        "X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
        "\n",
        "## check our changes\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CriG6VzVSjcK"
      },
      "source": [
        "## Encoding categorical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AhSpdQWeSsFh"
      },
      "source": [
        "### Encoding the Independent Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To categorize France, Germany and Spain from test set into categories, we will use vector based catagory. For example, Germany = 001.\n",
        "We dont use Germany=0, Spain=1 and France=3, because then model may think that they are the priorities of countires.\n",
        "\n",
        "Also we will change \"purchased\" column into vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [0.0 1.0 0.0 30.0 54000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 35.0 58000.0]\n",
            " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "## create object of ColumnTransformer\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])] , remainder='passthrough')\n",
        "# [0] = column key of first column, remainder='passthrough' makes sure numerical (age and salary) are not changed. \n",
        "X = np.array(ct.fit_transform(X))\n",
        "# fit_transform will be used for fitting and transforming data\n",
        "# X must be numpy array (requirement by python). Hence use np.array()\n",
        "\n",
        "print(X)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DXh8oVSITIc6"
      },
      "source": [
        "### Encoding the Dependent Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encoding for Purchased column => use of LabelEncoder\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "# convert NO and YES from column to vector\n",
        "y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1 0 0 1 1 0 1 0 1]\n"
          ]
        }
      ],
      "source": [
        "print(y)\n",
        "\n",
        "# 0=no, 1=yes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qb_vcgm3qZKW"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Question: Should we do splitting of dataset BEFORE feature scaling, or AFTER?\n",
        "Answer: BEFORE\n",
        "Why: Test set is supposed to be brand new set, on which you will apply your ML model. If we apply feature scaling before, it will apply mean & standard deviation of all values, including one in test set. \n",
        "This is done to prevent information leakage of test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will make 4 set of sets (2 for training, 2 for test)\n",
        "X_train = matrix of features of the training set\n",
        "X_test = matrix of features of test set\n",
        "y_train = dependent variable of training set (column 'Purchased')\n",
        "y_test = all the Purchased decision of customers in the training set\n",
        "\n",
        "=> Recommended data split: 80% in training set, 20% in test set (using test_size = 0.2 below)\n",
        "=> By setting random_state to a fixed number (in this case, 1), the split will always be the same every time you run the code, ensuring reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# use of function 'train_test_split'\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 35.0 58000.0]]\n"
          ]
        }
      ],
      "source": [
        "print(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.0 1.0 0.0 30.0 54000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ],
      "source": [
        "print(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1 0 0 1 1 0 1]\n"
          ]
        }
      ],
      "source": [
        "print(y_train)\n",
        "# 8 purchased decisions. These will correspond to same 8 customers in X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(y_test)\n",
        "# 2 purchased decisions. These will correspond to same 2 customers in X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TpGqbS4TqkIR"
      },
      "source": [
        "## Feature Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feature scaling is a preprocessing step in machine learning where the range of features (input variables) is normalized to ensure they have comparable magnitudes. \n",
        "\n",
        "Main goal: to transform all values of set into limited range, so thats its easy to compare.\n",
        "\n",
        "----\n",
        "Standardisation:\n",
        "$ X_{stand}=\\frac{x\\ -\\ mean(x)}{standard\\ deviation(x)} $\n",
        "\n",
        "Subtract each value of your feature, by mean of all values of your feature and divide by standard deviation.\n",
        "\n",
        "----------\n",
        "\n",
        "Normalisation:\n",
        "$ X_{norm}=\\frac{x\\ -\\ min(x)}{max(x)\\ -\\ min(x)} $\n",
        "\n",
        "Subtract each value of your feature by minimum value and divide by (maximum value of feature - minimum value of feature)\n",
        "\n",
        "----\n",
        "\n",
        "Question: Which one to use from above 2?\n",
        "\n",
        "Answer: \n",
        "\n",
        "=> Normalisation is recommeded when you have normal distribution in most of your features. \n",
        "\n",
        "=> Standardisation works well all the time.\n",
        "\n",
        "We will not do feature scaling for country name, since they are already in range of -3 to +3 (e.g. 1.0 0.0 0.0 for Spain).\n",
        "But yes for salary, we should do feature scaling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# use of \"StandardScaler\"\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "\n",
        "#fit our standarisation tool on our training set. Ignore first column and take next 2.\n",
        "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n",
        "X_test[:, 3:] = sc.fit_transform(X_test[:, 3:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.0 0.0 1.0 -0.19159184384578554 -1.0781259408412427]\n",
            " [0.0 1.0 0.0 -0.014117293757057874 -0.07013167641635415]\n",
            " [1.0 0.0 0.0 0.566708506533324 0.6335624327104545]\n",
            " [0.0 0.0 1.0 -0.3045301939022487 -0.30786617274297906]\n",
            " [0.0 0.0 1.0 -1.901801144700799 -1.4204636155515822]\n",
            " [1.0 0.0 0.0 1.1475343068237056 1.2326533634535488]\n",
            " [0.0 1.0 0.0 1.4379472069688966 1.5749910381638883]\n",
            " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757337]]\n"
          ]
        }
      ],
      "source": [
        "print(X_train)\n",
        "\n",
        "# in output, age and salary is between -3 and +3. Country name is unchanged."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.0 1.0 0.0 30.0 54000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ],
      "source": [
        "print(X_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "data_preprocessing_tools.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
