{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new variable which will contain dataset\n",
    "dataset = pd.read_csv('Data.csv')\n",
    "\n",
    "## create feature (X) and dependent variables (y) (in data.csv, country+age+salary is features, purchased is depedent variable)\n",
    "X = dataset.iloc[:, :-1].values  \n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takecare of missing data\n",
    "method 1: manually delete empty data \n",
    "& \n",
    "\n",
    "method 2: replace missing values by average of all values in that column (Use of sklearn class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# create instance of the class & replace all missing values in dataset with mean of the feature itself\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')  \n",
    "# apply imputer object on features\n",
    "imputer.fit(X[:, 1:3])\n",
    "# perform replcement for empty values for 2 columns (age and salary)\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
    "\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takecare of missing categorical data\n",
    "\n",
    "In our dataset, Country has categories. For better ML calcualtions, we have to change countries to numbers (e.g Germany=1, Spain=2, France=3). However, future ML models can think that these are priorities, which is incorrect. \n",
    "\n",
    "Hence, encode them differently. (Use of OneHotEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# create object. [0] means transform only country column (first column)\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "\n",
    "# perform fit and transform\n",
    "X = np.array(ct.fit_transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding dependent variable\n",
    "\n",
    "Encode purchased column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "y = le.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting dataset into traning set and test set\n",
    "X_train (contains countries, age, salary in train set))<br>\n",
    "X_test (contains countries, age, salary in test set))<br>\n",
    "y_train (contains Purchased decisions in train set)<br>\n",
    "y_test (contains Purchased decisions in test set)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 0.2 means 20% dataset will go to test set, 80% to training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "### Standardisation (Works all the time, go for it when you are not sure which scaling to use)\n",
    "### Normalisation (Recommended when you have normal distribution in your features. Use only in specific scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()  # this will do standardisation\n",
    "\n",
    "# we take here column \"3:\", because since we changed to vector, first 3 columns are for country name. Hence, age column is 4th column now.\n",
    "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n",
    "\n",
    "# perform transformation on X_test. However, for X_test we will only do transform method.\n",
    "X_test[:, 3:] = sc.transform(X_test[:, 3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question: Should we do feature scaling AFTER splitting dataset or BEFORE splitting dataset? <br>Answer: After, because test set is supposed to be brand new untouched dataset. Feature scaling basically performs some actions on dataset like mean. This can result in information leakage.\n",
    "\n",
    "#### transform v/s fit_transform\n",
    "fit_transform on training data: learns and applies the scaling. <br>\n",
    "transform on test data: applies the same scaling learned from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
